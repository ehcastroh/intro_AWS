{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 6px solid#003262;\" />\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src= \"/content/datax_logos/DataX_blue_logo.png\" align=\"center\" width=\"35%\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "# INTRODUCTION TO AWS: ELASTICS COMPUTE CLOUD (EC2)\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Author List:** Elias Castro Hernandez\n",
    "\n",
    "**About (TL/DR):** The following collection of notebooks introduces developers and data scientists to web development using Flask. Flask is one of many available web server gateway interface (WSGI) tools that enable rapid and scalable websites and apps with a relatively accessible learning curve. The barebones capacity of Flask is particularly valuable when prototyping and iterating upon products, services, and machine learning applications.\n",
    "\n",
    "**Learning Goal(s):** Gain an understanding of how to utilize available libraries and packages to quickly build products and services -- in real-life settings, using web-first methodology, driven by data, and end-to-end. In particular, learn how to build a bare-bones flask environment for handling large scale email automation tasks.\n",
    "\n",
    "**Target User:** Data scientists, applied machine learning engineers, and developers\n",
    "\n",
    "**Prerequisite Knowledge:** (1) Python, (2) HTML, and (3) CSS\n",
    "\n",
    "**Copyright:** Content curation has been used to expediate the creation of the following learning materials. Credit and copyright belong to the content creators used in facilitating this content. Please support the creators of the resources used by frequenting their sites, and social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "#### CONTENTS\n",
    "\n",
    "> #### [PART 1: PREREQUISITE KNOWLEDGE AND REQUIREMENTS](#Part_1)\n",
    "\n",
    "> #### PART 2: SETTING UP AWS SERVER\n",
    "\n",
    "> #### PART 3: WRAP UP AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APPENDIX\n",
    "\n",
    "> #### REQUIRED PACKAGES: INSTALLATION\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='Part_1'></a>\n",
    "\n",
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "\n",
    "#### PART 1\n",
    "<br>\n",
    "\n",
    "## PREREQUISITE **KNOWLEDGE** AND **REQUIREMENTS**\n",
    "\n",
    "\n",
    "In order to create a barebones website that communicates with existing API's to generate emails, we will need to connect several components. Specifically, we will to spin up an [**AWS server**](https://aws.amazon.com/) account, create an [**Elastic Compute Cloud (EC2)**](https://aws.amazon.com/ec2/) instance, link our [**Flask**](https://flask.palletsprojects.com/en/1.1.x/) environment to the [**Google Sheets API**](https://developers.google.com/sheets/api), and then send emails using [**smtplib**](https://docs.python.org/3/library/smtplib.html) via either a local or remote server. There are many theoretical and applied topics that underlie the various components of technology stack just mentioned. For the sake of focusing on the implementation, none of the notebooks in this series will deepdive into any of the required components. Intead, the content is purposefully accessible and designed for selflearning with the assumption that the user is familiar with the above concepts. In case you are not yet ready but want to follow along, following are some helpful links to help you with some of the prerequisites.<br>\n",
    "\n",
    "\n",
    "##### **PYTHON**\n",
    "This notebook and the executable files are built using [Python](https://www.python.org/) and relies on common Python packages (e.g. [NumPy](https://numpy.org/)) for operation. If you have a lot of programming experience in a different language (e.g. C/C++/Matlab/Java/Javascript), you will likely be fine, otherwise:\n",
    "\n",
    "> [**Python (EDX free)**](https://www.edx.org/course?search_query=python)<br>\n",
    "> [**Python (Coursera free)**](https://www.coursera.org/search?query=python)\n",
    "\n",
    "\n",
    "##### **AMAZON ELASTIC COMPUTE CLOUD (EC2)**\n",
    "Amazon Web Services (AWS) is the leading cloud service provider in both revenue and market share. As such, this notebook and associated scripts are written to function with AWS's Elastic Compute Clud (EC2) service. However, several competitors such as Microsoft's [**Azure**](https://azure.microsoft.com/en-us/) and Google's [**GCP**](https://cloud.google.com/) provide entry pricing and free credits to test out their service. If you elect to use AWS and are not fully familiar with the process of setting a web server, the following are a few approachable resources for using AWS:\n",
    "\n",
    "> [**How to Get Started with Amazon EC2**](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html#how-to-get-started) <br>\n",
    "\n",
    "\n",
    "##### **AMAZON SIMPLE EMAIL SERVICE (SES)**\n",
    "Amazon Simple Email Service (SES) is a cost-effective way for sending and receiving emails. In case you want to learn more, see the following accessible resource:\n",
    "\n",
    "> [**EC2 INSTANCE USING LINUX**](https://docs.aws.amazon.com/ses/) <br>\n",
    "\n",
    "\n",
    "##### **SMTPLIB**\n",
    "Python's **smtplib** package is a powerful library that facilitates the sending of emails from either local or remote servers. This free package is a great alternative to services such as AWS's SES.\n",
    "\n",
    "> [**Real Python: Sending Emails Using SMTPLIB**](https://realpython.com/python-send-email/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "#### PART 2\n",
    "<br>\n",
    "\n",
    "## SETTING UP **AWS SERVER**\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"font-size:12px; font-family:FreeMono; font-weight: 100; font-stretch:ultra-condensed; line-height: 1.0; color:#2A2C2B\">\n",
    "    <img src=\"/images/replace_with_actual.png\" align=\"center\" width=\"40%\" padding=\"20\"><br>\n",
    "    <br>\n",
    "    System Architecture and Data Flows\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Amazon Web Services, like all of their competitors, offers service credits to students in an effort to facilitate learning and promote adoption. If you are interested in requesting AWS student credits, click [here](https://aws.amazon.com/blogs/aws/aws-educate-credits-training-content-and-collaboration-for-students-educators/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "#### PART 3\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **BASELINE**, MODEL COMPARISON. AND **RESULTS**\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **LOAD PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-156b2aa54aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m                             \u001b[0;31m# tokenizing and ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchtext_data\u001b[0m                \u001b[0;31m# data preprocessing utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m                              \u001b[0;31m# get data --> https://pytorch.org/text/datasets.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreebankWordDetokenizer\u001b[0m  \u001b[0;31m# parse throught data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import ludwig                                               # main library\n",
    "from ludwig.api import LudwigModel                          # machine learning \n",
    "import torch\n",
    "import torch.utils.data as data                             # tokenizing and ngrams\n",
    "from torchtext import data as torchtext_data                # data preprocessing utilities\n",
    "from torchtext import datasets                              # get data --> https://pytorch.org/text/datasets.html\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer  # parse throught data structure\n",
    "import pandas as pd\n",
    "import pandas.util.testing as tm\n",
    "import yaml                                                 # issuing Ludwig commands\n",
    "import logging                                              # error and operation logs\n",
    "from pprint import pprint                                   # human readable print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **LOAD DATA**\n",
    "To perform sentiment analysis on the SST dataset, we will be using [**Torchtext**](https://pytorch.org/text/) to gather and split the data, while [**Pytorch**](https://pytorch.org/docs/stable/index.html) is used for perprocessing. These tools are being used for simplicity. If you elect to build from the ground up, you fill find the SST data in **stanfordSentimentTreebank** or **stanfordSentimentTreebankRaw** in the data folder. The following provides and gentle introduction to DIY [**data preprocessing**](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79), while [**mlexplained**](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/) has a great intro to Torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize Vocab tensor objects\n",
    "text = torchtext_data.Field()\n",
    "label = torchtext_data.Field(sequential=False)  # no tokenization since False\n",
    "\n",
    "\n",
    "# get data and split into testing and training --> https://pytorch.org/text/datasets.html#sst\n",
    "train_data, val_data, test_data = datasets.SST.splits(\n",
    "    text,\n",
    "    label,\n",
    "    fine_grained=True,\n",
    "    train_subtrees=True,  #use all subtrees in the training set\n",
    ")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in trange(len(train_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(\n",
    "        vars(train_data[i])[\"text\"]\n",
    "    )\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    if add_eos_token:\n",
    "        seq = [50256] + seq\n",
    "    seq = torch.tensor(seq, device=device, dtype=torch.long)\n",
    "    x.append(seq)\n",
    "    y.append(class2idx[vars(train_data[i])[\"label\"]])\n",
    "train_dataset = Dataset(x, y)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in trange(len(test_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(\n",
    "        vars(test_data[i])[\"text\"]\n",
    "    )\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    if add_eos_token:\n",
    "        seq = [50256] + seq\n",
    "    seq = torch.tensor(seq, device=device, dtype=torch.long)\n",
    "    test_x.append(seq)\n",
    "    test_y.append(class2idx[vars(test_data[i])[\"label\"]])\n",
    "test_dataset = Dataset(test_x, test_y)\n",
    "\n",
    "discriminator_meta = {\n",
    "    \"class_size\": len(idx2class),\n",
    "    \"embed_size\": discriminator.embed_size,\n",
    "    \"pretrained_model\": pretrained_model,\n",
    "    \"class_vocab\": class2idx,\n",
    "    \"default_class\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "#### APPENDIX\n",
    "\n",
    "<br>\n",
    "\n",
    "## **INSTALLATION:** LUDWIG, PYTORCH, TORCHTEXT, ALTAIR\n",
    "\n",
    "<br>\n",
    "\n",
    "**About Ludwig:** Ludwig is an open source deep learning framework buit atop of TensorFlow that allows users to rapidly train and iterate on state of the art deep learning models with only a few lines of code.\n",
    "\n",
    "**About Pytorch:** Pytorch is an open source machine library commonly used in language and vision applications. Pytorch is fast through it's use of tensors and optimized algorithms, while also being accessible.\n",
    "\n",
    "**About Torchtext:** Torchtext makes common natural language preprocessing easier and convenient -- particularly because of built-in functionality that loads, pads and batches data into whatever your prefered deep learning framework requires.\n",
    "\n",
    "**About Altair:** MAY DELETE ALTAIR IF NO VALUE\n",
    "\n",
    "<br>\n",
    "\n",
    "### **General Information**\n",
    "\n",
    "> [**INSTALL LUDWIG**](https://uber.github.io/ludwig/getting_started/)\n",
    "\n",
    "> [**INSTALL PYTORCH**](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "> [**INSTALL TORCHTEXT**](https://pytorch.org/text/index.html)\n",
    "\n",
    "> [**INSTALL ALTAIR**](https://altair-viz.github.io/getting_started/installation.html)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "\n",
    "### **Install LUDWIG**\n",
    "\n",
    "\n",
    "**To Install Ludwig:**\n",
    "\n",
    "```bash\n",
    "# recommended approach\n",
    "pip install ludwig \n",
    "```\n",
    "OR\n",
    "```bash\n",
    "# install it by building the source code from the repository:\n",
    "git clone git@github.com:uber/ludwig.git\n",
    "cd ludwig\n",
    "virtualenv -p python3 venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "python setup.py install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Ludwig is developed and tested with Python 3 in mind. To install Python 3:\n",
    "\n",
    "```bash\n",
    "sudo apt install python3  # on ubuntu\n",
    "brew install python3      # on mac\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### **Install PYTORCH**\n",
    "\n",
    "```python\n",
    "# prefered method \n",
    "conda install -c pytorch pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### **Install TORCHTEXT**\n",
    "\n",
    "```python\n",
    "# prefered method \n",
    "conda install -c pytorch torchtext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### **Install ALTAIR**\n",
    "\n",
    "```bash\n",
    "# in a new terminal\n",
    "pip install altair vega_datasets \n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "```python\n",
    "conda install -c conda-forge altair vega_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "enable_chalkboard": true,
   "height": "90%",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
