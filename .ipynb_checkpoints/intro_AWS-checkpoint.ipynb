{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid#0B0B0B;\" />\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src= \"/content/innovation_engineering.png\" align=\"center\" width=\"30%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## INTRODUCTION TO AMAZON WEB SERVICES\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Author List:** Elias Castro Hernandez\n",
    "\n",
    "**About (TL/DR):** The following collection of notebooks introduces developers and data scientists to \n",
    "cloud computing services offered by Amazon Web Services (AWS). Cloud computing eliminates the need for large quantities of computing technology and on-site data centers and allow users to pay for what they use. As the leading cloud computing platform in both revenue and market share, AWS provides secure and affordable services for its users. \n",
    "\n",
    "\n",
    "**Learning Goal(s):** Learn why AWS is useful and how to use its core services\n",
    "\n",
    "**Target User:** Data scientists and developers\n",
    "\n",
    "**Prerequisite Knowledge:** none\n",
    "\n",
    "**Copyright:** Content curation has been used to expediate the creation of the following learning materials. Credit and copyright belong to the content creators used in facilitating this content. Please support the creators of the resources used by frequenting their sites, and social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "#### CONTENTS: CORE AWS SERVICES\n",
    "\n",
    "> #### PART 1: SIMPLE STORAGE SERVICE\n",
    "\n",
    "> #### PART 2: ELASTIC CLOUD COMPUTE\n",
    "\n",
    "> #### PART 3: MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### APPENDIX\n",
    "\n",
    "> #### [**ACCESS AWS SERVER**](https://aws.amazon.com/)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "\n",
    "#### PART 1\n",
    "<br>\n",
    "\n",
    "## **SIMPLE STORAGE SERVICE (S3)**\n",
    "\n",
    "Amazon's storage service, S3 allows users to store and retrieve unlimited amounts of data, storing files of any type.\n",
    "\n",
    "##### **STRUCTURE**\n",
    "Object < Folder < Object\n",
    "Bucket: folders of files \n",
    "Folder: subfolders\n",
    "Object: file you would like to store; may be placed in buckets or folders\n",
    "\n",
    "\n",
    "##### BENEFITS OF S3\n",
    "\n",
    "##### **1. S3 ACCESS POINTS**\n",
    "<img src= \"https://d1.awsstatic.com/re19/Westeros/Diagram_S3_Access_Points.fa88c474dc1073aede962aaf3a6af2d6b02be933.png\" align = center width = 100%>\n",
    "\n",
    "##### **2. HIGHLY SCALABLE & FLEXIBLE**\n",
    "- can decide data lifestyle\n",
    "- can transfer data securely and quickly at Amazon's high speed internal network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid#0B0B0B;\" />\n",
    "\n",
    "#### PART 2\n",
    "<br>\n",
    "\n",
    "##  **ELASTIC COMPUTE CLOUD (EC2)**\n",
    "\n",
    "\n",
    "With an [**AWS server**](https://aws.amazon.com/) account, we will access the Elastic Cloud Computer Service (EC2), [**Elastic Compute Cloud (EC2)**](https://aws.amazon.com/ec2/). Think of a basic desktop computer. EC2 is pretty much that, without the hardware and can practically function as an unlimited set of physical computers. This service provides scalable computing capacity which allows you to develop and deploy applications quickly and effectively. You can launch as many servers as you need, configure security and networking, and scale changes whenever you would like. \n",
    "\n",
    "For reference, here is Amazon's guide for using EC2:\n",
    "> [**How to Get Started with Amazon EC2**](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html#how-to-get-started) <br>\n",
    "\n",
    "##### **LAUNCHING AN EC2 INSTANCE**\n",
    "An EC2 instance is a virtual server that allows you to run applications on AWS infrastructure. Each server functions as \"computer\" would.\n",
    "STEPS (EDIT THIS!):\n",
    "1. Open EC2 in AWS.\n",
    "2. Adjust the necessary settings.\n",
    "3. Create a key pair to log back into the server or be able to SSH into the instance.\n",
    "4. Download the key pair so that you have access to the instance.\n",
    "5. Deploy instance.\n",
    "\n",
    "TROUBLESHOOTING:\n",
    "Connectivity Issues may be a result of\n",
    "1. security group access\n",
    "2. network access\n",
    "3. control list rules\n",
    "4. route tables\n",
    "5. internet gateways\n",
    "\n",
    "The benefit of EC2 over a standard dekstop computer is its autoscaling function and its elastic IP address that belongs to the AWS account instead of a physical hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "#### PART 3\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **MACHINE LEARNING**\n",
    "\n",
    "\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
    "\n",
    "pattern recognition\n",
    "\n",
    "You can choose from pre-trained AI services for computer vision, language, recommendations, and forecasting; Amazon SageMaker to quickly build, train and deploy machine learning models at scale; or build custom models with support for all the popular open-source frameworks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### SageMaker\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-156b2aa54aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m                             \u001b[0;31m# tokenizing and ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchtext_data\u001b[0m                \u001b[0;31m# data preprocessing utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m                              \u001b[0;31m# get data --> https://pytorch.org/text/datasets.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreebankWordDetokenizer\u001b[0m  \u001b[0;31m# parse throught data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import ludwig                                               # main library\n",
    "from ludwig.api import LudwigModel                          # machine learning \n",
    "import torch\n",
    "import torch.utils.data as data                             # tokenizing and ngrams\n",
    "from torchtext import data as torchtext_data                # data preprocessing utilities\n",
    "from torchtext import datasets                              # get data --> https://pytorch.org/text/datasets.html\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer  # parse throught data structure\n",
    "import pandas as pd\n",
    "import pandas.util.testing as tm\n",
    "import yaml                                                 # issuing Ludwig commands\n",
    "import logging                                              # error and operation logs\n",
    "from pprint import pprint                                   # human readable print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize Vocab tensor objects\n",
    "text = torchtext_data.Field()\n",
    "label = torchtext_data.Field(sequential=False)  # no tokenization since False\n",
    "\n",
    "\n",
    "# get data and split into testing and training --> https://pytorch.org/text/datasets.html#sst\n",
    "train_data, val_data, test_data = datasets.SST.splits(\n",
    "    text,\n",
    "    label,\n",
    "    fine_grained=True,\n",
    "    train_subtrees=True,  #use all subtrees in the training set\n",
    ")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in trange(len(train_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(\n",
    "        vars(train_data[i])[\"text\"]\n",
    "    )\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    if add_eos_token:\n",
    "        seq = [50256] + seq\n",
    "    seq = torch.tensor(seq, device=device, dtype=torch.long)\n",
    "    x.append(seq)\n",
    "    y.append(class2idx[vars(train_data[i])[\"label\"]])\n",
    "train_dataset = Dataset(x, y)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in trange(len(test_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(\n",
    "        vars(test_data[i])[\"text\"]\n",
    "    )\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    if add_eos_token:\n",
    "        seq = [50256] + seq\n",
    "    seq = torch.tensor(seq, device=device, dtype=torch.long)\n",
    "    test_x.append(seq)\n",
    "    test_y.append(class2idx[vars(test_data[i])[\"label\"]])\n",
    "test_dataset = Dataset(test_x, test_y)\n",
    "\n",
    "discriminator_meta = {\n",
    "    \"class_size\": len(idx2class),\n",
    "    \"embed_size\": discriminator.embed_size,\n",
    "    \"pretrained_model\": pretrained_model,\n",
    "    \"class_vocab\": class2idx,\n",
    "    \"default_class\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "### **Install ALTAIR**\n",
    "\n",
    "```bash\n",
    "# in a new terminal\n",
    "pip install altair vega_datasets \n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "```python\n",
    "conda install -c conda-forge altair vega_datasets\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "enable_chalkboard": true,
   "height": "90%",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
